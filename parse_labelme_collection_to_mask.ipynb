{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse LabelMe Annotations and Images to Mask, train.csv, and val.csv\n",
    "\n",
    "## Goal: \n",
    "1. Download collection folder from google drive \n",
    "2. Create another folder to store 'masks', 'labels', 'images'\n",
    "3. Create train, test and validation csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import xmltodict\n",
    "import sys\n",
    "import errno\n",
    "import shutil\n",
    "import csv\n",
    "import random\n",
    "import gdown\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_exist(directory):\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        print ('folder: ', directory, ' is not existed, please check')\n",
    "\n",
    "def check_and_create_folder(directory):\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "        print ('folder: ', directory, 'is existed, do you want to remove it')\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "        print ('create ', directory)\n",
    "        \n",
    "# Copy Images and Labels dir to target dir\n",
    "def copy_folder(src, dest):\n",
    "    try:\n",
    "        print (src)\n",
    "        print (dest)\n",
    "        shutil.copytree(src, dest)\n",
    "    except OSError as e:\n",
    "        # If the error was caused because the source wasn't a directory\n",
    "        if e.errno == errno.ENOTDIR:\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print('Directory not copied. Error: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2mask(xml_path, product_list, height, width):\n",
    "    \"\"\"\n",
    "    Convert xml file to a label image\n",
    "\n",
    "    Read a xml file, and generate a gray image.\n",
    "    There are few kinds values in a image. For number of kinds, see 'classes_list':\n",
    "    0: background\n",
    "\n",
    "    Args:\n",
    "        xml_path: xml path\n",
    "    Returns:\n",
    "        1-channel lable image\n",
    "    \"\"\"\n",
    "    # check xml file exits\n",
    "    file_exist = os.path.isfile(xml_path)    # True\n",
    "    \n",
    "    # create empty mask\n",
    "    mask = np.zeros([height, width], dtype = np.uint8)\n",
    "    \n",
    "    # load xml file info\n",
    "    if file_exist:\n",
    "        with open(xml_path) as fd:\n",
    "            label_dict = xmltodict.parse(fd.read())\n",
    "    else:\n",
    "        print (xml_path, 'does not exist')\n",
    "        return mask\n",
    "\n",
    "    # check objects labelled in xml file\n",
    "    if 'object' in label_dict['annotation']:\n",
    "        # only single object in xml file \n",
    "        if type(label_dict['annotation']['object']).__name__ != \"list\":\n",
    "            # extract object region and draw mask with pixel value 255\n",
    "            try:\n",
    "                object_ = label_dict['annotation']['object']\n",
    "                if object_['name'] in product_list and object_['deleted'] == '0':\n",
    "                    poly_vertice = []\n",
    "                    for pts_idx in object_['polygon']['pt']:\n",
    "                        poly_vertice.append([int(pts_idx['x']), int(pts_idx['y'])])\n",
    "                    poly_vertice = np.array(poly_vertice, np.int32)\n",
    "                    object_index = product_list.index(object_['name'])\n",
    "                    cv2.fillConvexPoly(mask, poly_vertice, object_index)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "        # multiple objects in xml file\n",
    "        else:\n",
    "            # extract objects region and draw mask with pixel value corressponding to class index\n",
    "            try:\n",
    "                for object_ in label_dict['annotation']['object']:\n",
    "                    if object_['name'] in product_list and object_['deleted'] == '0':\n",
    "                        # print(\"object matched\")\n",
    "                        poly_vertice = []\n",
    "                        for pts_idx in object_['polygon']['pt']:\n",
    "                            poly_vertice.append([int(pts_idx['x']), int(pts_idx['y'])])\n",
    "                        poly_vertice = np.array(poly_vertice, np.int32)\n",
    "                        object_index = product_list.index(object_['name']) + 1\n",
    "                        cv2.fillConvexPoly(mask, poly_vertice, object_index)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "           \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset using \"gdown\" python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1bULPUhuQ6BHjVgn_owuswF9qrOCC1fLO\n",
      "To: /home/samliu/code/pytorch-shoes-segmentation/shoes_collection.zip\n",
      "11.3MB [00:02, 4.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading dataset.\n"
     ]
    }
   ],
   "source": [
    "# modify info here by your case\n",
    "collection_url = 'https://drive.google.com/uc?id=1bULPUhuQ6BHjVgn_owuswF9qrOCC1fLO'\n",
    "collection_name = 'shoes_collection'\n",
    "source_annotations_dir = os.path.join(collection_name, 'Annotations/users/lab605/shoes_new')\n",
    "source_images_dir = os.path.join(collection_name, 'Images/users/lab605/shoes_new')\n",
    "target_output_dir = 'shoes_dataset_folder'\n",
    "classes_list = ['left_shoe', 'right_shoe']\n",
    "width = 640\n",
    "height = 480\n",
    "training_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "\n",
    "if not os.path.isdir(collection_name):\n",
    "    gdown.download(collection_url, output=collection_name + '.zip', quiet=False)\n",
    "    zip1 = ZipFile(collection_name + '.zip')\n",
    "    zip1.extractall('./')\n",
    "    zip1.close()\n",
    "print 'Finished downloading dataset.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Path of Annotations, Images, Output Foloder, and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('create ', 'shoes_dataset_folder')\n",
      "('create ', 'shoes_dataset_folder/masks')\n",
      "shoes_collection/Annotations/users/lab605/shoes_new\n",
      "shoes_dataset_folder/labels\n",
      "shoes_collection/Images/users/lab605/shoes_new\n",
      "shoes_dataset_folder/images\n"
     ]
    }
   ],
   "source": [
    "# do not modify\n",
    "target_masks_dir = os.path.join(target_output_dir, 'masks')\n",
    "target_images_dir = os.path.join(target_output_dir, 'images')\n",
    "target_labels_dir = os.path.join(target_output_dir, 'labels')\n",
    "\n",
    "check_folder_exist(source_annotations_dir)\n",
    "check_folder_exist(source_images_dir)\n",
    "\n",
    "check_and_create_folder(target_output_dir)\n",
    "check_and_create_folder(target_masks_dir)\n",
    "\n",
    "copy_folder(source_annotations_dir, target_labels_dir)\n",
    "copy_folder(source_images_dir, target_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Mask Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_xmls = os.listdir(target_labels_dir)\n",
    "for xml_name in sorted(label_xmls):\n",
    "    # create mask depending on xml\n",
    "    xml_path = os.path.join(target_labels_dir, xml_name)\n",
    "    mask = xml2mask(xml_path, classes_list, height, width)\n",
    "\n",
    "    # create mask file path\n",
    "    mask_file_name = xml_name.split(\"xml\")[0] + \"png\"\n",
    "    save_path = os.path.join(target_masks_dir, mask_file_name)\n",
    "    # print('save_path' + save_path)\n",
    "    cv2.imwrite(save_path, mask,[int(cv2.IMWRITE_JPEG_QUALITY), 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far, you will see there is a new dataset folder created in project root.\n",
    "### Inside the dataset folder, there are 3 sub folders include 'labels', 'masks', 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random generate train, test and validation csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = os.listdir(target_images_dir)\n",
    "mask_files = os.listdir(target_masks_dir)\n",
    "dataset = []\n",
    "\n",
    "# match images and masks into dataset\n",
    "for image_file in image_files:\n",
    "    image_name = image_file.split(\".\")[0]\n",
    "    for mask_file in mask_files:\n",
    "        mask_name = mask_file.split(\".png\")[0]\n",
    "        if mask_name == image_name:\n",
    "            dataset.append('images/' + image_file + ',masks/' + mask_file)\n",
    "\n",
    "# split dataset into training and testing parts\n",
    "random.shuffle(dataset)\n",
    "train_dataset = dataset[:int((len(dataset)+1)*training_ratio)] #Remaining X% to training set\n",
    "val_dataset = dataset[int(len(dataset)*training_ratio+1):int(len(dataset)*(training_ratio+val_ratio))] #Splits X% data to test set\n",
    "test_dataset = dataset[int(len(dataset)*(training_ratio+val_ratio)+1):]\n",
    "\n",
    "# write train.csv\n",
    "with open((target_output_dir + '/train.csv'), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)    \n",
    "    for train_data in train_dataset:\n",
    "        data = train_data.split(\",\")\n",
    "        writer.writerow([data[0], data[1]])\n",
    "\n",
    "# write val.csv\n",
    "with open((target_output_dir + '/val.csv'), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)    \n",
    "    for val_data in val_dataset:\n",
    "        data = val_data.split(\",\")\n",
    "        writer.writerow([data[0], data[1]])\n",
    "        \n",
    "        \n",
    "# write test.csv\n",
    "with open((target_output_dir + '/test.csv'), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)    \n",
    "    for test_data in test_dataset:\n",
    "        data = test_data.split(\",\")\n",
    "        writer.writerow([data[0], data[1]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "description": "Define, train, and test the classic LeNet with the Python interface.",
  "example_name": "Learning LeNet",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "priority": 2
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
